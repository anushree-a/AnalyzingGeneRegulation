{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def converter(sequence):\n",
    "    new_sequence=[]\n",
    "    for i in range(len(sequence)):\n",
    "        if sequence[i]=='a' or sequence[i]=='A':\n",
    "            new_sequence.append('T')\n",
    "        elif sequence[i]=='c' or sequence[i]=='C':\n",
    "            new_sequence.append('G')\n",
    "        elif sequence[i]=='g' or sequence[i]=='G':\n",
    "            new_sequence.append('C')\n",
    "        elif sequence[i]=='t' or sequence[i]=='T':\n",
    "            new_sequence.append('A')\n",
    "        \n",
    "            \n",
    "    str1 = ''.join(new_sequence)\n",
    "    str1=''.join(reversed(str1))\n",
    "    \n",
    "    return str1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abhinav/anaconda2/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from DeepBind_Control.ipynb\n",
      "importing Jupyter notebook from Pipeline_Control.ipynb\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import keras\n",
    "import warnings\n",
    "import import_ipynb\n",
    "import DeepBind_Control as db22\n",
    "import deeplift\n",
    "import json\n",
    "from deeplift.conversion import kerasapi_conversion\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import model_from_json\n",
    "keras_model = model_from_json(open(\"/home/abhinav/Downloads/trying_1_8_21_newstuff/models/model_['ENCFF183UQD_H3K4me1'].json\").read())\n",
    "keras_model.load_weights(\"/home/abhinav/Downloads/trying_1_8_21_newstuff/modelweights/model_['ENCFF183UQD_H3K4me1']_weights.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deeplift.layers import NonlinearMxtsMode\n",
    "import deeplift.conversion.kerasapi_conversion as kc\n",
    "reload(deeplift.layers)\n",
    "reload(deeplift.conversion.kerasapi_conversion)\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200001\n",
      "14161\n"
     ]
    }
   ],
   "source": [
    "from Bio import SeqIO\n",
    "\n",
    "file1='/home/abhinav/Downloads/trying_1_8_21_newstuff/sliding_window_big_test_chr1.fa'\n",
    "file0='/home/abhinav/Downloads/trying_1_8_21_newstuff/inter_fa/class0_11files_only1821_test.fa'\n",
    "\n",
    "\n",
    "list0=[]\n",
    "list1=[]\n",
    "target1 = []\n",
    "target2 = []\n",
    "#making dataframe for train data\n",
    "for record in SeqIO.parse(file1, \"fasta\"):\n",
    "    if 'N' not in str(record.seq):\n",
    "            sequence=str(record.seq)+\"NNNNNNNNNN\"+str(converter(record.seq))\n",
    "            list1.append(sequence)\n",
    "            target1.append(1)    \n",
    "for record in SeqIO.parse(file0, \"fasta\"):\n",
    "    if 'N' not in str(record.seq):\n",
    "            sequence=str(record.seq)+\"NNNNNNNNNN\"+str(converter(record.seq))\n",
    "            list0.append(sequence)\n",
    "            target2.append(0)\n",
    "print len(list1)\n",
    "print len(list0)\n",
    "targets = target1[:60000] + target2\n",
    "sequences= list1[:60000]+list0\n",
    "# targets = target1+target2\n",
    "# sequences = list1+list0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[1. 0. 0. 0.]\n",
      "  [1. 0. 0. 0.]\n",
      "  [1. 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. 1.]\n",
      "  [0. 0. 0. 1.]\n",
      "  [0. 0. 0. 1.]]\n",
      "\n",
      " [[1. 0. 0. 0.]\n",
      "  [1. 0. 0. 0.]\n",
      "  [0. 1. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 1. 0.]\n",
      "  [0. 0. 0. 1.]\n",
      "  [0. 0. 0. 1.]]\n",
      "\n",
      " [[1. 0. 0. 0.]\n",
      "  [0. 1. 0. 0.]\n",
      "  [0. 0. 0. 1.]\n",
      "  ...\n",
      "  [1. 0. 0. 0.]\n",
      "  [0. 0. 1. 0.]\n",
      "  [0. 0. 0. 1.]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[1. 0. 0. 0.]\n",
      "  [0. 0. 1. 0.]\n",
      "  [0. 0. 1. 0.]\n",
      "  ...\n",
      "  [0. 1. 0. 0.]\n",
      "  [0. 1. 0. 0.]\n",
      "  [0. 0. 0. 1.]]\n",
      "\n",
      " [[1. 0. 0. 0.]\n",
      "  [1. 0. 0. 0.]\n",
      "  [1. 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. 1.]\n",
      "  [0. 0. 0. 1.]\n",
      "  [0. 0. 0. 1.]]\n",
      "\n",
      " [[0. 1. 0. 0.]\n",
      "  [0. 0. 1. 0.]\n",
      "  [0. 1. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 1. 0.]\n",
      "  [0. 1. 0. 0.]\n",
      "  [0. 0. 1. 0.]]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(74161, 410, 4)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ohe_sequences = np.array([db22.one_hot_encode(s) for s in sequences])   \n",
    "print ohe_sequences\n",
    "ohe_sequences.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "method_to_model = OrderedDict()\n",
    "for method_name, nonlinear_mxts_mode in [\n",
    "    #The genomics default = rescale on conv layers, revealcance on fully-connected\n",
    "    ('rescale_conv_revealcancel_fc', NonlinearMxtsMode.DeepLIFT_GenomicsDefault),\n",
    "    ('rescale_all_layers', NonlinearMxtsMode.Rescale),\n",
    "    ('revealcancel_all_layers', NonlinearMxtsMode.RevealCancel),\n",
    "    ('grad_times_inp', NonlinearMxtsMode.Gradient),\n",
    "    ('guided_backprop', NonlinearMxtsMode.GuidedBackprop)]:\n",
    "    method_to_model[method_name] = kc.convert_model_from_saved_files(\n",
    "        h5_file=\"//home/abhinav/Downloads/trying_1_8_21_newstuff/modelweights/model_['ENCFF578UBP_H3K27ac']_weights.h5\",\n",
    "        json_file=\"/home/abhinav/Downloads/trying_1_8_21_newstuff/models/model_['ENCFF578UBP_H3K27ac'].json\",\n",
    "        nonlinear_mxts_mode=nonlinear_mxts_mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deeplift.util import compile_func\n",
    "model_to_test = method_to_model['rescale_conv_revealcancel_fc']\n",
    "deeplift_prediction_func = compile_func([model_to_test.get_layers()[0].get_activation_vars()],\n",
    "                                         model_to_test.get_layers()[-1].get_activation_vars())\n",
    "original_model_predictions = keras_model.predict(ohe_sequences, batch_size=200)\n",
    "converted_model_predictions = deeplift.util.run_function_in_batches(\n",
    "                                input_data_list=[ohe_sequences],\n",
    "                                func=deeplift_prediction_func,\n",
    "                                batch_size=200,\n",
    "                                progress_update=None)\n",
    "print(\"maximum difference in predictions:\",np.max(np.array(converted_model_predictions)-np.array(original_model_predictions)))\n",
    "assert np.max(np.array(converted_model_predictions)-np.array(original_model_predictions)) < 10**-5\n",
    "predictions = converted_model_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Compiling scoring functions\")\n",
    "method_to_scoring_func = OrderedDict()\n",
    "for method,model in method_to_model.items():\n",
    "    print(\"Compiling scoring function for: \"+method)\n",
    "    method_to_scoring_func[method] = model.get_target_contribs_func(find_scores_layer_idx=0,\n",
    "                                                                    target_layer_idx=-2)\n",
    "    \n",
    "#To get a function that just gives the gradients, we use the multipliers of the Gradient model\n",
    "gradient_func = method_to_model['grad_times_inp'].get_target_multipliers_func(find_scores_layer_idx=0,\n",
    "                                                                              target_layer_idx=-2)\n",
    "print(\"Compiling integrated gradients scoring functions\")\n",
    "integrated_gradients10_func = deeplift.util.get_integrated_gradients_function(\n",
    "    gradient_computation_function = gradient_func,\n",
    "    num_intervals=10)\n",
    "method_to_scoring_func['integrated_gradients10'] = integrated_gradients10_func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "background = OrderedDict([('A', 0.25), ('C', 0.25), ('G', 0.25), ('T', 0.25)])\n",
    "\n",
    "from collections import OrderedDict\n",
    "method_to_task_to_scores = OrderedDict()\n",
    "for method_name, score_func in method_to_scoring_func.items():\n",
    "    print(\"on method\",method_name)\n",
    "    method_to_task_to_scores[method_name] = OrderedDict()\n",
    "    for task_idx in [0]:\n",
    "        scores = np.array(score_func(\n",
    "                    task_idx=task_idx,\n",
    "                    input_data_list=[ohe_sequences],\n",
    "                    input_references_list=[\n",
    "                     np.array([background['A'],\n",
    "                               background['C'],\n",
    "                               background['G'],\n",
    "                               background['T']])[None,None,:]],\n",
    "                    batch_size=200,\n",
    "                    progress_update=None))\n",
    "        assert scores.shape[2]==4\n",
    "        #The sum over the ACGT axis in the code below is important! Recall that DeepLIFT\n",
    "        # assigns contributions based on difference-from-reference; if\n",
    "        # a position is [1,0,0,0] (i.e. 'A') in the actual sequence and [0.3, 0.2, 0.2, 0.3]\n",
    "        # in the reference, importance will be assigned to the difference (1-0.3)\n",
    "        # in the 'A' channel, (0-0.2) in the 'C' channel,\n",
    "        # (0-0.2) in the G channel, and (0-0.3) in the T channel. You want to take the importance\n",
    "        # on all four channels and sum them up, so that at visualization-time you can project the\n",
    "        # total importance over all four channels onto the base that is actually present (i.e. the 'A'). If you\n",
    "        # don't do this, your visualization will look very confusing as multiple bases will be highlighted at\n",
    "        # every position and you won't know which base is the one that is actually present in the sequence!\n",
    "        scores = np.sum(scores, axis=2)\n",
    "        method_to_task_to_scores[method_name][task_idx] = scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualize scores + ground-truth locations of motifs\n",
    "%matplotlib inline\n",
    "from deeplift.visualization import viz_sequence\n",
    "\n",
    "# for task, idx in [(0,731), #illustrates failure of grad*inp, integrated grads, deeplift-rescale\n",
    "#                   (1,197)  #illustrates non-specific firing of guided backprop\n",
    "#                  ]:\n",
    "for task, idx in [(0,41000\n",
    "                  ) #illustrates failure of grad*inp, integrated grads, deeplift-rescale\n",
    "                          #illustrates non-specific firing of guided backprop\n",
    "                 ]:\n",
    "    print(\"Scores for task\",task,\"for example\",idx)\n",
    "    for method_name in [\n",
    "                        'grad_times_inp',\n",
    "                        'guided_backprop',\n",
    "                        'integrated_gradients10',\n",
    "                        'rescale_all_layers', 'revealcancel_all_layers',\n",
    "                        'rescale_conv_revealcancel_fc',\n",
    "                        'rescale_conv_revealcancel_fc_multiref_10'\n",
    "                        ]:\n",
    "        scores = method_to_task_to_scores[method_name][task]\n",
    "        scores_for_idx = scores[idx]\n",
    "        original_onehot = ohe_sequences[idx]\n",
    "        scores_for_idx = original_onehot*scores_for_idx[:,None]\n",
    "        print(method_name)\n",
    "#         highlight = {'blue':[\n",
    "#                 (embedding.startPos, embedding.startPos+len(embedding.what))\n",
    "#                 for embedding in data.embeddings[idx] if 'GATA_disc1' in embedding.what.getDescription()],\n",
    "#                 'green':[\n",
    "#                 (embedding.startPos, embedding.startPos+len(embedding.what))\n",
    "#                 for embedding in data.embeddings[idx] if 'TAL1_known1' in embedding.what.getDescription()]}\n",
    "        viz_sequence.plot_weights(scores_for_idx, subticks_frequency=10)                                                                                                                                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74161/74161 [==============================] - 8s 113us/step\n"
     ]
    }
   ],
   "source": [
    "keras_model.compile(loss='binary_crossentropy', optimizer='Adam', metrics=['accuracy'])\n",
    "scores_array= keras_model.predict(ohe_sequences, batch_size=None, verbose=1, steps=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.40195018],\n",
       "       [0.40073362],\n",
       "       [0.47356182],\n",
       "       ...,\n",
       "       [0.57574594],\n",
       "       [0.3632302 ],\n",
       "       [0.73801726]], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"sliding_window_scores_big_UBP.pkl\", 'wb') as f:\n",
    "    pickle.dump(scores_array.tolist(), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
